<html>

<head>
	<!-- for gitbook -->
	<title>Week 8 - Deep Learning from Scratch @ Opencampus</title>
	<meta charset="UTF-8">
	<meta name="description" content="week 8.">
	<meta name="keywords" content="Deep Learning, Opencampus, week 8">
	<meta name="author" content="Luca Palmieri">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="icon" type="image/png" href="../../res/icon/dl-icon.png">
	<meta property="og:image" content="../../res/icon/dl-icon.png">
	<!-- until here -->
	<base target="_blank">
	<link rel="stylesheet" href="../../css/reveal.css">
	<link rel="stylesheet" href="../../css/theme/night.css">
	<link rel="stylesheet" href="../../css/s.css">
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? '../../css/print/pdf.css' : '../../css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<!-- WELCOME TO THE COURSE -->
			<section>
				<h3>Deep Learning <br>from Scratch</h3>

				<h2 class="slidetitle" style="padding-bottom: 4rem">
					Machine Learning Strategy 1 & 2
				</h2>
				<p>Course starts soon..</p><br>
			</section>

			<section>
				<p class="slidetitle">Quiz</p><br>
				<p>We will start with the usual quiz</p>
				<p>The quiz link: <br>
					<a href="https://forms.office.com/r/r6jzQW4hQB" target="_blank">Quiz Link</a><br>
					It will be copied in Mattermost and in the Zoom chat.
				</p>
			</section>

			<section>
				<section>
					<p class="slidetitle">Final Presentations</p><br>
					<p>
						We divide in 2 days to avoid sessions which are too long.<br>
						Each group will have 20 minutes:<br>15/20 of presentation, 5/10 of discussion<br>
						<b>Both sessions are mandatory</b>
					</p>
				</section>
				<section>
					<p class="slidetitle">Final Presentations, Part 1</p><br>
					<p><b>04th of July</b></p>
					<ol>
						<li>
							<b>Deep Finance - Hedging and Calibration</b>(16.00-16.30)<br>
						</li>
						<li>
							<b>Hot Autonomous Wheels: </b>(16.30-17.00)<br>
						</li>
						<li>
							<b>Age Detection </b>(17.00-17.30)<br>
						</li>
					</ol>
					</p>
				</section>
				<section>
					<p class="slidetitle">Final Presentations, Part 2</p><br>
					<p><b>11th of July</b></p>
					<ol>
						<li>
							<b>NFT Search Engine </b>(16.00-16.30)<br>
						</li>
						<li>
							<b>GANs </b>(16.30-17.00)<br>
						</li>
						<li>
							<b>Text classification on arXiv abstracts</b>(16.40-17.00)<br>
						</li>
					</ol>
					</p>
				</section>
			</section>

			<!-- RNN/LSTM -->
			<section>
				<section>
					<p class="slidetitle">RNN, LSTM and Transformers</p>
					<img src="../../res/advml/lstm.png" class="r-stretch">
					<p>A leap into language processing</p>
				</section>
				<section>
					<p class="slidetitle">What makes Recurrent Networks so special? [1]</p>
					<p class="fragment"><i>they [Neural Networks] accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). [..] The core reason that recurrent nets are more
							exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both.</i> [1]</p>
				</section>
				<section>
					<p class="slidetitle">What does Long Short Term Memory means?</p>
					<p class="slidewithalotoftext"><i>[LSTM] are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp; Schmidhuber (1997) [2] [..] LSTMs are explicitly designed to avoid the long-term
							dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!</i> [3]</p>
				</section>
				<section>
					<p class="slidetitle">Natural Language Processing Application [4]</p>
					<iframe src="https://distill.pub/2019/memorization-in-rnns/#ar-demo" width=100% height="500rem;" style="background: white"></iframe>
				</section>
				<section>
					<p class="slidetitle">Compare text classifiers</p>
					<img src="../../res/d1/w5/text.png" width=68%>
					<p class="source"><a href="http://iphome.hhi.de/samek/pdf/ICIP2018_4.pdf">Slide from Samek's presentation at ICIP 2018 [5]</a></p>
				</section>
				<section>
					<p class="slidetitle">Transformers</p>
					<img src="../../res/tra1.png" width=44% class="r-stretch">
				</section>
				<section>
					<p class="slidetitle">Attention</p>
					<p>
						<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">
							What it is and why we need it - with animations
						</a>
					</p>
				</section>
				<section>
					<p class="slidetitle">Try it yourself</p>
					</p>
				</section>
				<section>
					<p class="slidetitle">GPT-3</p>
				</section>
				<section>
					<p class="slidetitle">DALL-E</p>
				</section>
				<section>
					<p class="slidetitle">RNN/LSTM Bibliography</p>
					<p class="slidewithalotoftext" style="text-align: left">
						<br>[1] <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The unreasonable Effectiveness of RNN - Andrej Karpathy</a>
						<br>[2] <a href="https://www.bioinf.jku.at/publications/older/2604.pdf"><i>Long Short Term Memory</i>, Hochreiter et al. (1997)</a>
						<br>[3] <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTMs - Colah's Blog</a>
						<br>[4] <a href="https://distill.pub/2019/memorization-in-rnns/">Memorization RNN/LSTM</a>
						<br>[5] <a href="http://iphome.hhi.de/samek/pdf/ICIP2018_4.pdf">Slide from Samek's presentation at ICIP 2018</a>
						<br>[6] <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All you Need (Paper)</a>
						<br>[7] <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Online Article with Animations</a>
						<br>[8] <a href="https://www.youtube.com/watch?v=iH-wmtxHunk">Video Explanation of a transformer paper</a>
						<br>[9] <a href="https://transformer.huggingface.co/">Writing with Transformers</a>
						<br>[10] <a href="https://transformer.huggingface.co/model/arxiv-nlp">ArXiv NLP model checkpoint</a>

						<br>Additional Resources:
						<br>[6] <a href="http://joshvarty.github.io/VisualizingRNNs/">Animation RNN</a>
						<br>[7] <a href="http://blog.echen.me/">Detailed Explanation, LSTM</a>
						<br>[8] <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">RNN representation</a>
					</p>
				</section>
				<section>
					<p>If you like these, take the</p>
					<a href="https://edu.opencampus.sh/courses/215"><p class="slidetitle huge">Machine Learning with Tensorflow</p></a>
					<p>or</p>
					<a href="https://edu.opencampus.sh/courses/233"><p class="slidetitle huge">Natural Language Processing with Transformer</p></a>
					<p>course</p>
				</section>
			</section>

			<!-- Questions -->
			<section>
				<p class="slidetitle">Open Questions (15 mins)</p><br>
				<ol class="slidewithalotoftext">
					<li>Can you think of a task where machines has not yet surpassed human level performances, but most likely in the future will?</li>
					<li>What is the difference between random errors and sistematic errors on the labelling of your training data?</li>
					<li>Can you explain the difference between multi-task and end-to-end learning? Can you make an example for both of them?</li>
				</ol>
			</section>

			<!-- ANSWERS -->
			<section>
				<section>
					<p class="slidetitle">DISCUSSION AND ANSWERS</p><br>
				</section>
			</section>





			<!-- HAUSAUFGABE -->
			<section>
				<p class="slidetitle">Next Session after Holidays</p>
				<ul>
					<li>Enjoy the holidays</li>
					<li>Work on your project</li>
					<li>Start preparing the presentation</li>
				</ul>
			</section>

		</div>
	</div>
	<script src="../../js/reveal.js"></script>
	<script>
		Reveal.initialize();
	</script>
</body>

</html>
