<html>

<head>
	<!-- for gitbook -->
	<title>Week 8 - Deep Learning from Scratch @ Opencampus</title>
	<meta charset="UTF-8">
	<meta name="description" content="week 8.">
	<meta name="keywords" content="Deep Learning, Opencampus, week 8">
	<meta name="author" content="Luca Palmieri">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="icon" type="image/png" href="../../res/icon/dl-icon.png">
	<meta property="og:image" content="../../res/icon/dl-icon.png">
	<!-- until here -->
	<base target="_blank">
	<link rel="stylesheet" href="../../css/reveal.css">
	<link rel="stylesheet" href="../../css/theme/night.css">
	<link rel="stylesheet" href="../../css/s.css">
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? '../../css/print/pdf.css' : '../../css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<!-- WELCOME TO THE COURSE -->
			<section>
				<h3>Deep Learning <br>from Scratch</h3>

				<h2 class="slidetitle" style="padding-bottom: 4rem">
					Machine Learning Strategy 1 & 2
				</h2>
				<p>Course starts soon..</p><br>
			</section>

			<section>
				<p class="slidetitle">Quiz</p><br>
				<p>We will start with the usual quiz</p>
				<p>The quiz link: <br>
					<a href="https://forms.office.com/r/g8Kwt1iAV9" target="_blank">Quiz Link</a><br>
					It will be copied in Mattermost and in the Zoom chat.
				</p>
			</section>

			<section>
				<section>
					<p class="slidetitle">Presentation of the final project</p>
					<ul class="slidewithalotoftext">
						<li>Each group will make a presentation. Ideally, the presentation would be divided among all members of the group.</li>
						<li>Live demo or example are very welcome</li>
						<li>Each group should take 15-20 minutes. After the presentation there will be a small discussion.</li>
						<li>The structure of the presentation is not fixed, nor it is its content. However, there are some suggestions to have similar structures.</li>
					</ul>
				</section>
				<section>
					<p class="slidetitle">Guidelines for the presentation</p>
					<ul class="slidewithreallyalotoftext">
					  <li><b>Project</b>: short description of the project and the motivation behind.</li>
					  <li><b>Architecture</b>: what architecture did you use, how many layers, which function (you can be technical on this part)</li>
						<li><b>Results</b>: how it works? can you quantify results or show some visualizations?</li>
						<li><b>Baselines</b>:
						<ul>
							<li>is there a simple baseline with which you can compare? It can be as simple as the mean of the data for time series prediction, or a fixed value of 0.5 for the accuracy in a binary classification problem.</li>
							<li>can you measure human performances on the task? can you personally take some samples from the test set and solve the problem yourself, and quantify the results?</li>
						</ul>
						</li>
						<li><b>(*Optional) Missing:</b> is there something you missed to improve the project? Time, material, knowledge, data, computational power?</li>
						<li><b>(*Optional) Future works:</b> how could the project be improved or extended?</li>
					</ul>
				</section>
				<section>
					<p class="slidetitle">Sharing is caring</p>
					If not otherwise discussed with a single group,
					we will add your project to our <a href="https://opencampus-sh.github.io/oc-ml-projects/">Project Page</a>
					<ul class="slidewithalotoftext">
						<li><b>Code</b>: Provide a link to your repository if you have one,
							and also some short instructions to reproduce it if needed<br>
							Please check that the code is clean (no testing or commented code)
							and has comments or text fields to understand it!</li>
						<li><b>Data:</b> mention if the data is public (with link),
							or if it is not possible to share.</li>
					</ul>
				</section>
			</section>

			<section>
				<section>
					<p class="slidetitle">Schedule</p><br>
					<p>
						We divide in 2 days to avoid sessions which are too long.<br>
						Each group will have up to 30 minutes:<br>15/20 of presentation, 5/10 of discussion<br>
						<b>Both sessions are mandatory</b>
					</p>
				</section>
				<section>
					<p class="slidetitle">Final Presentations, Part 1</p><br>
					<p><b>04th of July</b></p>
					<ol>
						<li>
							<b>Deep Finance - Hedging and Calibration</b> (16.00-16.30)<br>
						</li>
						<li>
							<b>Hot Autonomous Wheels</b> (16.30-17.00)<br>
						</li>
						<li>
							<b>Age Detection</b> (17.00-17.30)<br>
						</li>
					</ol>
					</p>
				</section>
				<section>
					<p class="slidetitle">Final Presentations, Part 2</p><br>
					<p><b>11th of July</b></p>
					<ol>
						<li>
							<b>NFT Search Engine </b>(16.00-16.30)<br>
						</li>
						<li>
							<b>GANs </b>(16.30-17.00)<br>
						</li>
						<li>
							<b>Text classification on arXiv abstracts</b> (17.00-17.30)<br>
						</li>
					</ol>
					</p>
				</section>
			</section>

			<!-- Questions -->
			<section>
				<p class="slidetitle">Open Questions (15 mins)</p><br>
				<ol class="slidewithalotoftext">
					<li>Can you think of a task where machines has not yet surpassed human level performances, but most likely in the future will?</li>
					<li>What is the difference between random errors and sistematic errors on the labelling of your training data?</li>
					<li>Can you explain the difference between multi-task and end-to-end learning? Can you make an example for both of them?</li>
				</ol>
			</section>

			<!-- RNN/LSTM -->
			<section>
				<section>
					<p class="slidetitle">RNN, LSTM and Transformers</p>
					<img src="../../res/advml/lstm.png" class="r-stretch">
					<p>A leap into language processing</p>
				</section>
				<section>
					<p class="slidetitle">What makes Recurrent Networks so special? [1]</p>
					<p class="fragment"><i>they [Neural Networks] accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). [..] The core reason that recurrent nets are more
							exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both.</i> [1]</p>
				</section>
				<section>
					<p class="slidetitle">What does Long Short Term Memory means?</p>
					<p class="slidewithalotoftext"><i>[LSTM] are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp; Schmidhuber (1997) [2] [..] LSTMs are explicitly designed to avoid the long-term
							dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!</i> [3]</p>
				</section>
				<section>
					<p class="slidetitle">Natural Language Processing Application [4]</p>
					<iframe src="https://distill.pub/2019/memorization-in-rnns/#ar-demo" width=100% height="500rem;" style="background: white"></iframe>
				</section>
				<section>
					<p class="slidetitle">Compare text classifiers</p>
					<img src="../../res/d1/w5/text.png" width=68%>
					<p class="source"><a href="http://iphome.hhi.de/samek/pdf/ICIP2018_4.pdf">Slide from Samek's presentation at ICIP 2018 [5]</a></p>
				</section>
				<section>
					<p class="slidetitle">Transformers</p>
					<img src="../../res/tra1.png" width=44% class="r-stretch">
				</section>
				<section>
					<p class="slidetitle">Attention</p>
					<p>
						<iframe src="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" width=100% height="500rem;" style="background: white"></iframe>
					</p>
				</section>
				<section>
					<p class="slidetitle">Try it yourself</p>
					<a href="https://transformer.huggingface.co/">Using huggingface models!</a><br>
					(<a href="https://transformer.huggingface.co/model/arxiv-nlp">one also on arXiv abstracts!</a> )
					</p>
				</section>
				<section>
					<p class="slidetitle">GPT-3</p>
					<iframe src="https://gpt3.website/" width=100% height="500rem;" style="background: white"></iframe>
					<a href="https://arxiv.org/abs/2005.14165v4">Research Paper</a>
				</section>
				<section>
					<p class="slidetitle">DALL-E</p>
					<a href="https://openai.com/dall-e-2">OpenAI Model</a><br>
					<a href="https://arxiv.org/abs/2204.06125">Research Paper</a>
					<p class="slidetitle">DALL-E mini</p>
					<a href="https://huggingface.co/spaces/dalle-mini/dalle-mini">Demo Page</a><br>
					<a href="https://huggingface.co/dalle-mini/dalle-mini">Model Info</a>
				</section>
				<section>
					<p class="slidetitle">EleutherAI</p>
					<iframe src="https://www.eleuther.ai/" width=100% height="500rem;" style="background: white"></iframe>
					<a href="https://github.com/EleutherAI">Github Org</a>
				</section>
				<section>
					<p class="slidetitle">RNN/LSTM Bibliography</p>
					<p class="slidewithalotoftext" style="text-align: left">
						<br>[1] <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The unreasonable Effectiveness of RNN - Andrej Karpathy</a>
						<br>[2] <a href="https://www.bioinf.jku.at/publications/older/2604.pdf"><i>Long Short Term Memory</i>, Hochreiter et al. (1997)</a>
						<br>[3] <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTMs - Colah's Blog</a>
						<br>[4] <a href="https://distill.pub/2019/memorization-in-rnns/">Memorization RNN/LSTM</a>
						<br>[5] <a href="http://iphome.hhi.de/samek/pdf/ICIP2018_4.pdf">Slide from Samek's presentation at ICIP 2018</a>
						<br>[6] <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All you Need (Paper)</a>
						<br>[7] <a href="https://www.youtube.com/watch?v=iH-wmtxHunk">Video Explanation of a transformer paper</a>

						<br>Additional Resources:
						<br>[8] <a href="http://joshvarty.github.io/VisualizingRNNs/">Animation RNN</a>
						<br>[9] <a href="http://blog.echen.me/">Detailed Explanation, LSTM</a>
						<br>[10] <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">RNN representation</a>
					</p>
				</section>
				<!-- <section>
					<p>If you like these, take the</p>
					<a href="https://edu.opencampus.sh/courses/215"><p class="slidetitle huge">Machine Learning with Tensorflow</p></a>
					<p>or</p>
					<a href="https://edu.opencampus.sh/courses/233"><p class="slidetitle huge">Natural Language Processing with Transformer</p></a>
					<p>course</p>
				</section> -->
			</section>



			<!-- ANSWERS -->
			<section>
				<section>
					<p class="slidetitle">DISCUSSION AND ANSWERS</p><br>
				</section>
			</section>





			<!-- HAUSAUFGABE -->
			<section>
				<p class="slidetitle">Next Session after Holidays</p>
				<ul>
					<li>Enjoy the holidays</li>
					<li>Work on your project</li>
					<li>Start preparing the presentation</li>
				</ul>
			</section>

		</div>
	</div>
	<script src="../../js/reveal.js"></script>
	<script>
		Reveal.initialize();
	</script>
</body>

</html>
