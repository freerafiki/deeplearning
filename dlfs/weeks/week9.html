<html>

<head>
	<!-- for gitbook -->
	<title>Week 9 - Deep Learning @ Opencampus</title>
	<meta charset="UTF-8">
	<meta name="description" content="week 9">
	<meta name="keywords" content="Deep Learning, Opencampus, week 9">
	<meta name="author" content="Luca Palmieri">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="icon" type="image/png" href="../res/icon/dl-icon.png">
	<meta property="og:image" content="../../res/icon/dl-icon.png">
	<!-- until here -->
	<base target="_blank">
	<link rel="stylesheet" href="../../css/reveal.css">
	<link rel="stylesheet" href="../../css/theme/night.css">
	<link rel="stylesheet" href="../../css/s.css">
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? '../../css/print/pdf.css' : '../../css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<!-- WELCOME TO THE COURSE -->
			<section>
				<h3>Deep Learning <br>from Scratch</h3>

				<h2 class="slidetitle" style="padding-bottom: 4rem">
					Overview of different Neural Networks
				</h2>
				<p>Course starts soon..</p><br>
			</section>

			<!-- PROJECT UPLOAD -->
			<section>
				<section>
					<p class="slidetitle">How to Upload Your Project:<br>1 - Choose</p><br>
					<img src="../../res/choose.png">
				</section>
				<section>
					<p class="slidetitle">How to Upload Your Project:<br>2 - Fill and Upload</p><br>
					<img src="../../res/upload.png">
				</section>
			</section>


			<!-- ANSWERS -->
			<section>
				<p class="slidetitle">Overview of different Networks</p>
				<ul>
					<li>FFNN</li>
					<li>Encoder/Decoder</li>
					<li>CNN</li>
					<li>RNN/LSTM</li>
					<li>GAN</li>
					<li>Transformer</li>
				</ul>
			</section>

			<section>
				<!-- FFNN -->
				<section>
					<p class="slidetitle">Feed Forward Neural Networks</p>
					<img class="r-stretch" src="../../res/advml/ffnn.png">
					<p>The network we saw during the course: <br>input layer, hidden layer(s), output layer</p>
				</section>
				<section>
					<p class="">Introduced in 1958 as the perceptron [1]</p>
					<img src="../../res/advml/peceptron.png" class="r-stretch">
					<p class="">Biologically inspired</p>
				</section>
				<section>
					<p class="slidetitle">Bibliography</p>
					<p class="slidewithalotoftext" style="text-align: left">
						<br>[1] <a href="https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf">Rosenblatt, Frank. <i>The perceptron: a probabilistic model for information storage and organization in the brain.</i> Psychological review 65.6 (1958):
							386.</a>
					</p>
				</section>
			</section>

			<!-- ENCODER -->
			<!-- AE/VAE -->
			<section>
				<section>
					<p class="slidetitle">Autoencoders</p>
					<img src="../../res/advml/ae.png" class="r-stretch">
					<p>What is an encoder doing?</p>
				</section>
				<section>
					<p class="slidewithalotoftext"><i>An autoencoder is a neural network that learns to copy its input to its output.</i> [1]</p>
					<p class="fragment slidewithreallyalotoftext"><i>It has an internal (hidden) layer that describes a code used to represent the input, and it is constituted by two main parts: an encoder that maps the input into the code, and a decoder that
							maps the code to a reconstruction of the original input.</i> [1]<br>
						<img src="../../res/advml/ae_schema.jpg" width=30%><br>
					</p>
					<p class="fragment slidewithalotoftext"><i>Often when people write autoencoders, the hope is that the middle layer h will take on useful properties in some compressed format. [2]</i></p>
				</section>
				<section>
					<p class="slidetitle">An example of when we do not need it [2,3]</p>
					<p class="fragment"><img src="../../res/advml/ae1.png" height=253rem;></p>
					<p class="fragment"><img src="../../res/advml/ae2.png" height=200rem;></p>
				</section>
				<section>
					<p class="slidetitle">An example of when we do need it [2,3]</p>
					<p class="fragment"><img src="../../res/advml/denoising.png" width=1600rem;></p>
					<p class="fragment"><img src="../../res/advml/denoising2.png" width=1600rem;></p>
				</section>
				<section>
					<p class="slidetitle">Bibliography</p>
					<p class="slidewithreallyalotoftext" style="text-align: left">
						<b>Articles:</b>
						<br>[1] <a href="https://en.wikipedia.org/wiki/Autoencoder">Autoencoder on Wikipedia</a>
						<br>[2] <a href="https://towardsdatascience.com/autoencoder-neural-networks-what-and-how-354cba12bf86">Article about autoencoders and denoising example (Towardsdatascience.com)</a>
						<br>[3] <a href="https://blog.keras.io/building-autoencoders-in-keras.html">Building Autoencoders in Keras</a>
						<br><b>Papers:</b>
						<br>[4] <a href="http://publications.idiap.ch/downloads/reports/2000/rr00-16.pdf">Bourlard, Hervé, and Yves Kamp. <i>Auto-association by multilayer perceptrons and singular value decomposition.</i> Biological cybernetics 59.4-5 (1988):
							291-294.</a>
						<br>[5] <a href="https://arxiv.org/pdf/1312.6114v10.pdf">Kingma, Diederik P., and Max Welling. <i>Auto-encoding variational bayes.</i> arXiv preprint arXiv:1312.6114 (2013).</a>
						<br>[6] <a href="http://machinelearning.org/archive/icml2008/papers/592.pdf">Vincent, Pascal, et al. <i>Extracting and composing robust features with denoising autoencoders.</i> Proceedings of the 25th international conference on Machine
							learning. ACM, 2008.</a>
					</p>
				</section>
			</section>

			<!-- CNN -->
			<section>
				<section>
					<p class="slidetitle">Convolutional Neural Networks</p>
					<img src="../../res/advml/cnn.png" class="r-stretch">
					<p>What is a convolution?</p>
				</section>
				<section>
					<p class="slidetitle">A Convolution, animated [1,2]</p>
					<img src="../../res/advml/full_padding_no_strides_transposed.gif" class="r-stretch">
				</section>
				<section>
					<p class="slidetitle">A Convolutional Neural Network explained [3]</p>
					<a href="https://youtu.be/aircAruvnKk?t=172"><img src="../../res/advml/vid_prev.png" width="1700rem" height="500px"></a>
				</section>
				<section>
					<p class="slidetitle">Compare image classifiers</p>
					<img src="../../res/d1/w5/horse.png" width=80%>
					<p class="source"><a href="http://iphome.hhi.de/samek/pdf/CERN2018.pdf">Slide from Samek's presentation at ICIP 2018 [4]</a></p>
				</section>
				<section>
					<p class="slidetitle">Not always as expected..</p>
					<img src="../../res/d1/w5/horse2.png" width=80%>
					<p class="source"><a href="http://iphome.hhi.de/samek/pdf/CERN2018.pdf">Slide from Samek's presentation at ICIP 2018 [4]</a></p>
				</section>
				<section>
					<p class="slidetitle">Deep Visualization Toolbox</p>
					<img src="../../res/d1/w5/deepvis_freckles.jpg" width="50%">
					<p class="source"><a href="http://yosinski.com/deepvis">Image from the website</a>[5]. Here also the <a href="https://github.com/yosinski/deep-visualization-toolbox">source code</a> of the Toolbox [6]</p>
				</section>
				<section>
					<p class="slidetitle"><a href="https://poloclub.github.io/cnn-explainer/">Bonus: CNN Explainer Demo [7]</a></p>
					<a href="https://poloclub.github.io/cnn-explainer/"><img src="../../res/advml/cnn_exp.png" width=100%></a>
				</section>
				<section>
					<p class="slidetitle"><a href="https://www.cs.ryerson.ca/~aharley/vis/conv/flat.html">Bonus: CNN Visualizer Demo [8]</a></p>
					<a href="https://www.cs.ryerson.ca/~aharley/vis/conv/flat.html"><img src="../../res/advml/cnn_vis.png" width=100%></a>
				</section>
				<section>
					<p class="slidetitle">CNN Bibliography</p>
					<p class="slidewithreallyalotoftext" style="text-align: left">

						<br>[1] <a href="https://arxiv.org/abs/1603.07285"><i>A guide to convolution arithmetic for deep learning</i>, Dumoulin et al. (2018)</a>
						<br>[2] <a href="https://github.com/vdumoulin/conv_arithmetic">Github repository for the animations related to the convolution arithmetic [1]</a>
						<br>[3] <a href="https://www.3blue1brown.com">3blue1brown: a great resource for math explanation and visualizations.</a>
						<br>[4] <a href="http://iphome.hhi.de/samek/pdf/CERN2018.pdf">Slides from Samek's presentation at ICIP 2018</a>
						<br>[5] <a href="http://yosinski.com/deepvis">Toolbox for Deep Visualization</a>
						<br>[6] <a href="https://github.com/yosinski/deep-visualization-toolbox">Github Repository of the Toolbox</a>
						<br>[7] <a href="https://poloclub.github.io/cnn-explainer/">CNN Explainer Demo: play with a CNN in your browser</a>
						<br>[8] <a href="https://www.cs.ryerson.ca/~aharley/vis/conv/flat.html">CNN Visualizer Demo: Flat 2D Visualization</a>
					</p>
				</section>
			</section>

			<!-- RNN/LSTM -->
			<section>
				<section>
					<p class="slidetitle">Recurrent Neural Networks and Long Short Term Memory</p>
					<img src="../../res/advml/lstm.png" class="r-stretch">
					<p>A leap into language processing</p>
				</section>
				<section>
					<p class="slidetitle">What makes Recurrent Networks so special? [1]</p>
					<p class="fragment"><i>they [Neural Networks] accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). [..] The core reason that recurrent nets are more
							exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both.</i> [1]</p>
				</section>
				<section>
					<p class="slidetitle">What does Long Short Term Memory means?</p>
					<p class="slidewithalotoftext"><i>[LSTM] are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter &amp; Schmidhuber (1997) [2] [..] LSTMs are explicitly designed to avoid the long-term
							dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!</i> [3]</p>
				</section>
				<section>
					<p class="slidetitle">Natural Language Processing Application [4]</p>
					<iframe src="https://distill.pub/2019/memorization-in-rnns/#ar-demo" width=100% height="500rem;" style="background: white"></iframe>
				</section>
				<section>
					<p class="slidetitle">Compare text classifiers</p>
					<img src="../../res/d1/w5/text.png" width=68%>
					<p class="source"><a href="http://iphome.hhi.de/samek/pdf/ICIP2018_4.pdf">Slide from Samek's presentation at ICIP 2018 [5]</a></p>
				</section>
				<section>
					<p class="slidetitle">RNN/LSTM Bibliography</p>
					<p class="slidewithalotoftext" style="text-align: left">
						<br>[1] <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The unreasonable Effectiveness of RNN - Andrej Karpathy</a>
						<br>[2] <a href="https://www.bioinf.jku.at/publications/older/2604.pdf"><i>Long Short Term Memory</i>, Hochreiter et al. (1997)</a>
						<br>[3] <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTMs - Colah's Blog</a>
						<br>[4] <a href="https://distill.pub/2019/memorization-in-rnns/">Memorization RNN/LSTM</a>
						<br>[5] <a href="http://iphome.hhi.de/samek/pdf/ICIP2018_4.pdf">Slide from Samek's presentation at ICIP 2018</a>
						<br>Additional Resources:
						<br>[6] <a href="http://joshvarty.github.io/VisualizingRNNs/">Animation RNN</a>
						<br>[7] <a href="http://blog.echen.me/">Detailed Explanation, LSTM</a>
						<br>[8] <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">RNN representation</a>
					</p>
				</section>
			</section>


			<!-- GAN -->
			<section>
				<section>
					<p class="slidetitle">Generative Adversarial Networks</p>
					<img src="../../res/advml/gan.png" class="r-stretch">
					<p>What does adversarial mean?</p>
				</section>
				<section>
					<p class="slidetitle">The birth of GAN</p>
					<a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets"><img src="../../res/d1/w6/gan.png"></a>
					<p class="source"><i><a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets">Generative Adversarial Nets</a></i>, Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
						Courville, Yoshua Bengio, <br><i>NIPS'14: Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2December 2014 Pages 2672–2680</i></p>
				</section>
				<section>
					<p class="slidetitle">Metaphor</p>
					<p class="slidewithalotoftext">
						<i>In the proposed adversarial nets framework, the generative model is pitted against an adversary: a discriminative model that learns to determine whether a sample is from the model distribution or the data distribution. The generative
							model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency.
							Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles.</i>
					</p>
				</section>
				<section>
					<p class="slidetitle">Bibliography</p>
					<p class="slidewithreallyalotoftext" style="text-align: left">
						<b>Articles:</b>
						<br>[1] <a href="https://papers.nips.cc/paper/5423-generative-adversarial-nets">Generative Adversarial Network, main Paper</a>
					</p>
				</section>
			</section>

			<!-- Transformers -->
			<section>
				<section>
					<p class="slidetitle">Transformer</p>
					<img src="../../res/advml/att.png" class="r-stretch">
					<p class="fragment">Where is this name coming from?</p>
					<p class="fragment">The biggest change is the attention mechanism</p>
				</section>
				<section>
					<p class="slidetitle-2">Attention</p>
					<p>
						<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">
							What it is and why we need it - with animations
						</a>
					</p>
				</section>
				<section>
					<p class="slidetitle">Bibliography</p>
					<p class="slidewithreallyalotoftext" style="text-align: left">
						<br>[1] <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All you Need (Paper)</a>
						<br>[2] <a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Online Article with Animations</a>
						<br>[3] <a href="https://www.youtube.com/watch?v=iH-wmtxHunk">Video Explanation of a transformer paper</a>
					</p>

				</section>
			</section>


			<!-- QUESTIONS -->
			<!--
			<section>
				<section>
					<p class="slidetitle">Peer Reviewed Project Check</p><br>
					<p>Today we make the check about the project status.</p>
				</section>
				<section>
					<p class="slidetitle">Remember from last week</p>
					<p class="slidewithalotoftext">We create Breakout Rooms, and in each Breakout Rooms there are two groups.</p>
					<p class="slidewithalotoftext">The first group has 5 minutes to explain the project, and the other group 2 minutes for feedback and questions. Then group switches and the second group explains and first gives feedback. In total it should
						take 14-15 minutes.</p>
					<p class="slidewithalotoftext">Afterwards we come back to the main session and each group reports shortly what they learned from the others. So listen carefully.</p>
				</section>
			</section> -->

			<!-- QUIZ -->
			<!-- <section>
				<p class="slidetitle">QUIZ (15 mins)</p><br>
				<p class="slidewithreallyalotoftext pb-1">
					1. How would you define human and optimal performances in your project? Are they the same?
				</p>
				<p class="slidewithreallyalotoftext pb-1">
					2. Do you expect your results to be close to one of them?
				</p>
				<p class="slidewithreallyalotoftext pb-1">
					3. We divide the project in 3 parts: <br>
					a) data preprocessing,
					b) building a model and getting it to work,
					c) fine tuning to achieve good results.
					Which part do you see as the easiest, and which as the most complex?
				</p>
				<p class="slidewithreallyalotoftext pb-1">
					4. Can you use transfer learning on your project? What should the model know?
				</p>
				<p class="slidewithreallyalotoftext pb-1">
					5. Can you think of a way to divide your objective in smaller tasks? (or the opposite)
				</p>
				<p class="slidewithreallyalotoftext pb-1">
					6. If you had 3 more months on your projects, what would be your next step? Assuming no money/time limitation, would you like to have more/better data, an expert validating your data, a more complex model, a more powerful machine to iterate
					the hyperparameter search, some time to test and apply different regularization techniques or something else?
					<br>
					<br>
				</p>
			</section> -->

			<!-- EXERCISES
			<section>
				<section>
					<p class="slidetitle">Hyperparameters Search</p>
					Can we make it easier?<br>
					<a href="https://github.com/freerafiki/Notebooks/blob/master/DeepLearning/In_search_of_the_hyperparameters.ipynb">Notebook</a>
				</section>
			</section>-->

			<!-- EXERCISES -->
			<!-- <section>
				<section>
					<p class="slidetitle">EXERCISE (15-20 mins)</p>
					We go through a tutorial for Time Series prediction using LSTM Networks from the Tensorflow Tutorials.
					<p class="source">
						<a href="https://github.com/freerafiki/Notebooks/blob/master/DeepLearning/Tutorial_LSTM_%5Bedit%5D.ipynb">The python Notebook</a><br>
						<br>
						Articles about RNN: <br>
						<a href="https://towardsdatascience.com/the-most-intuitive-and-easiest-guide-for-recurrent-neural-network-873c29da73c7">The Most Intuitive and Easiest Guide for Recurrent Neural Network</a><br>
						Articles about LSTM: <br>
						<a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">Illustrated Guide to LSTM’s and GRU’s: A step by step explanation</a><br>
						<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a>
					</p>
				</section>
			</section> -->

			<!-- HAUSAUFGABE -->
			<section>
				<p class="slidetitle">For the next week</p>
				<ul>
					<li>Finishing preparing the presentation</li>
					<li>You are done! Missing only the project!</li>
				</ul>
			</section>

		</div>
	</div>
	<script src="../../js/reveal.js"></script>
	<script>
		Reveal.initialize();
	</script>
</body>

</html>
